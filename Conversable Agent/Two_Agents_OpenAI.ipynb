{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7808e5f6-ac86-452c-8ab6-8640a69a2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install agentops\n",
    "#!pip install autogen\n",
    "#!pip install ag2[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e97a541-858d-48a7-8559-ee226f231826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.9'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen\n",
    "autogen.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f3c681-f283-48c4-b58e-8a6a80d8dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import agentops\n",
    "#agentops.init(\"Put your key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bef1002-ea29-4839-adfb-00d75dcaf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import CodeBlock,LocalCommandLineCodeExecutor\n",
    "\n",
    "working_dir=Path(\"coding\")\n",
    "working_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0b8496-7389-44d2-a145-a89cb4efc8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = LocalCommandLineCodeExecutor(work_dir=working_dir)\n",
    "\n",
    "#Agent 1\n",
    "code_executor_agent=ConversableAgent(\n",
    "name=\"code_executor_agent\",\n",
    "llm_config=False,\n",
    "code_execution_config={\"executor\":executor,},\n",
    "human_input_mode=\"NEVER\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5addba6-d3b0-46df-8d1b-7a23bc2870cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code writer agent's system message is to instruct the LLM on how to use the Jupyter code executor with IPython kernel.\n",
    "code_writer_system_message = \"\"\"\n",
    "You have been given coding capability to solve tasks using Python code.\n",
    "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user \n",
    "to execute.\n",
    "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, \n",
    "    download/read a file, \n",
    "    print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient \n",
    "    info is printed and the \n",
    "    task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
    "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the\n",
    "    task smartly.\n",
    "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses \n",
    "code, and which step uses your \n",
    "language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other\n",
    "feedback or perform any other \n",
    "action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which\n",
    "requires users to modify. \n",
    "Don't use a code block if it's not intended to be executed by the user.\n",
    "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block \n",
    "as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. \n",
    "Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7241ddb-1b6b-4c93-a5f2-37e40bb23174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent 2\n",
    "code_writer_agent=ConversableAgent(\n",
    "name=\"code_writer_agent\",\n",
    "llm_config={\"config_list\" :[{\"model\":\"gpt-4o-mini\",\"api_key\":\"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\"}]},\n",
    "code_execution_config=False,\n",
    "max_consecutive_auto_reply=2,\n",
    "human_input_mode=\"NEVER\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff65d56-3a00-4fc2-8003-a2b14790fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcode_executor_agent\u001b[0m (to code_writer_agent):\n",
      "\n",
      " A farmer has 48 apples. She wants to pack them into baskets, each holding exactly 7 apples. How many baskets will be completely full, and how many apples will remain unpacked?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chat_result = code_executor_agent.initiate_chat(\n\u001b[32m      4\u001b[39m     code_writer_agent, message=\u001b[33m\"\u001b[39m\u001b[33m A farmer has 48 apples. She wants to pack them into baskets, each holding exactly 7 apples. How many baskets will be completely full, and how many apples will remain unpacked?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m pprint.pprint(chat_result)\n\u001b[32m      8\u001b[39m agentops.end_session(\u001b[33m\"\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1509\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1507\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m         msg2send = \u001b[38;5;28mself\u001b[39m.generate_init_message(message, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(msg2send, recipient, silent=silent)\n\u001b[32m   1510\u001b[39m summary = \u001b[38;5;28mself\u001b[39m._summarize_chat(\n\u001b[32m   1511\u001b[39m     summary_method,\n\u001b[32m   1512\u001b[39m     summary_args,\n\u001b[32m   1513\u001b[39m     recipient,\n\u001b[32m   1514\u001b[39m     cache=cache,\n\u001b[32m   1515\u001b[39m )\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1163\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1161\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, recipient, is_sending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m     recipient.receive(message, \u001b[38;5;28mself\u001b[39m, request_reply, silent)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1167\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1271\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1271\u001b[39m reply = \u001b[38;5;28mself\u001b[39m.generate_reply(messages=\u001b[38;5;28mself\u001b[39m.chat_messages[sender], sender=sender)\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2864\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, **kwargs)\u001b[39m\n\u001b[32m   2862\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2863\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m2864\u001b[39m     final, reply = reply_func(\u001b[38;5;28mself\u001b[39m, messages=messages, sender=sender, config=reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   2866\u001b[39m         log_event(\n\u001b[32m   2867\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2868\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2872\u001b[39m             reply=reply,\n\u001b[32m   2873\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2191\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config)\u001b[39m\n\u001b[32m   2189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2190\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._oai_messages[sender]\n\u001b[32m-> \u001b[39m\u001b[32m2191\u001b[39m extracted_response = \u001b[38;5;28mself\u001b[39m._generate_oai_reply_from_client(\n\u001b[32m   2192\u001b[39m     client, \u001b[38;5;28mself\u001b[39m._oai_system_message + messages, \u001b[38;5;28mself\u001b[39m.client_cache\n\u001b[32m   2193\u001b[39m )\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2210\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache)\u001b[39m\n\u001b[32m   2207\u001b[39m         all_messages.append(message)\n\u001b[32m   2209\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2210\u001b[39m response = llm_client.create(\n\u001b[32m   2211\u001b[39m     context=messages[-\u001b[32m1\u001b[39m].pop(\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   2212\u001b[39m     messages=all_messages,\n\u001b[32m   2213\u001b[39m     cache=cache,\n\u001b[32m   2214\u001b[39m     agent=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2215\u001b[39m )\n\u001b[32m   2216\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\oai\\client.py:1168\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1167\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     response = client.create(params)\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\oai\\client.py:645\u001b[39m, in \u001b[36mOpenAIClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    643\u001b[39m     \u001b[38;5;28mself\u001b[39m._process_reasoning_model_params(params)\n\u001b[32m    644\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m response = create_or_parse(**params)\n\u001b[32m    646\u001b[39m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\oai\\client.py:448\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    447\u001b[39m     kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    450\u001b[39m     response_json = e.response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1149\u001b[39m         body=maybe_transform(\n\u001b[32m   1150\u001b[39m             {\n\u001b[32m   1151\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1152\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1153\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1154\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1155\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1156\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1157\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1158\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1159\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1160\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1161\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1162\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1163\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1164\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1165\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1166\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1168\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1169\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1174\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1175\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1185\u001b[39m             },\n\u001b[32m   1186\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   1187\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1188\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   1189\u001b[39m         ),\n\u001b[32m   1190\u001b[39m         options=make_request_options(\n\u001b[32m   1191\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1192\u001b[39m         ),\n\u001b[32m   1193\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1194\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1195\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m   1196\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda3\\envs\\agenticai\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "chat_result = code_executor_agent.initiate_chat(\n",
    "    code_writer_agent, message=\" A farmer has 48 apples. She wants to pack them into baskets, each holding exactly 7 apples. How many baskets will be completely full, and how many apples will remain unpacked?\"\n",
    ")\n",
    "\n",
    "pprint.pprint(chat_result)\n",
    "agentops.end_session(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47406a5d-9129-4b63-a5eb-af51700e82bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agenticai)",
   "language": "python",
   "name": "agenticai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
